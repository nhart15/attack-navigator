{
    "objects": [
        {
            "name": "Prevent",
            "description": "Stop all or part of the adversary's ability to conduct their operation as intended.\n\nPrevention focuses on stopping the adversary's ability to conduct their operations as intended. The defender can physically or virtually remove or disable resources, tighten security controls, or otherwise impair the adversary's ability to operate. A defender might prevent an adversary from operating to force them to reveal different, possibly more advanced, capabilities. Additionally, a defender can use Prevention to discourage the adversary from operating against a specific target. In this case, the defender may be attempting to encourage the adversary to focus elsewhere in the engagement environment.\n",
            "external_references": [
                {
                    "source_name": "mitre-atlas",
                    "url": "https://localhost:4200/tactics/EAP0003",
                    "external_id": "EAP0003"
                }
            ],
            "x_mitre_shortname": "prevent",
            "type": "x-mitre-tactic",
            "id": "x-mitre-tactic--798c2d7e-6da9-4d20-b01a-08dda01e4989",
            "created": "2022-03-30T12:50:48.721Z",
            "modified": "2022-03-30T12:50:48.721Z"
        },
        {
            "name": "Direct",
            "description": "Encourage or discourage the adversary from conducting their operation as intended.\n\nDirection focuses on moving the adversary towards or away from an intended path. This forced direction can be accomplished by removing or disabling some resources, while adding or enabling others. The defender can add decoy articles or otherwise manipulate the environment to attempt to elicit specific responses from the adversary. Additionally, the defender can tighten some security controls while leaving others overly permissive or weakened. Finally, the defender can physically move the adversary by moving threats from their intended environment and into a safe engagement environment.\nFor example, a suspicious email attachment can be moved from the intended target to an engagement environment for analysis. No matter how the Direction is achieved, the defender hopes to force the adversary to take unintended actions, or stop intended actions.\n",
            "external_references": [
                {
                    "source_name": "mitre-atlas",
                    "url": "https://localhost:4200/tactics/EAP0004",
                    "external_id": "EAP0004"
                }
            ],
            "x_mitre_shortname": "direct",
            "type": "x-mitre-tactic",
            "id": "x-mitre-tactic--d6430e09-9404-47d5-9731-9737f8d62bcb",
            "created": "2022-03-30T12:50:48.721Z",
            "modified": "2022-03-30T12:50:48.721Z"
        },
        {
            "name": "Disrupt",
            "description": "Impair an adversary's ability to conduct their operation as intended.\n\nDisruption is used to stop or discourage an adversary from conducting part or all of their mission.\nThis disruption may increase the time, skills, or resources needed for the adversary to accomplish a specific task.\nFor example, a defender may degrade network speeds as the adversary attempts to exfiltrate large blocks of data.\nAs a second example, the defender may manipulate the output of commonly used discovery commands to show targets that do not exist or to hide real targets.\n",
            "external_references": [
                {
                    "source_name": "mitre-atlas",
                    "url": "https://localhost:4200/tactics/EAP0005",
                    "external_id": "EAP0005"
                }
            ],
            "x_mitre_shortname": "disrupt",
            "type": "x-mitre-tactic",
            "id": "x-mitre-tactic--d9f7dc7f-48a4-4cf7-9417-ebb8df25fed9",
            "created": "2022-03-30T12:50:48.721Z",
            "modified": "2022-03-30T12:50:48.721Z"
        },
        {
            "name": "Reconnaissance",
            "description": "The adversary is trying to gather information they can use to plan\nfuture operations.\n\nReconnaissance consists of techniques that involve adversaries actively or passively gathering information that can be used to support targeting.\nSuch information may include details of the victim organizations machine learning capabilities and research efforts.\nThis information can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as using gathered information to obtain relevant ML artifacts, targeting ML capabilities used by the victim, tailoring attacks to the particular models used by the victim, or to drive and lead further Reconnaissance efforts.\n",
            "external_references": [
                {
                    "source_name": "mitre-atlas",
                    "url": "https://localhost:4200/tactics/AML.TA0002",
                    "external_id": "AML.TA0002"
                }
            ],
            "x_mitre_shortname": "reconnaissance",
            "type": "x-mitre-tactic",
            "id": "x-mitre-tactic--33e14ffb-49eb-4156-90df-782b9e44a3a3",
            "created": "2022-03-30T12:50:48.721Z",
            "modified": "2022-03-30T12:50:48.721Z"
        },
        {
            "name": "Resource Development",
            "description": "The adversary is trying to establish resources they can use to support operations.\n\nResource Development consists of techniques that involve adversaries creating,\npurchasing, or compromising/stealing resources that can be used to support targeting.\nSuch resources include machine learning artifacts, infrastructure, accounts, or capabilities.\nThese resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.\n",
            "external_references": [
                {
                    "source_name": "mitre-atlas",
                    "url": "https://localhost:4200/tactics/AML.TA0003",
                    "external_id": "AML.TA0003"
                }
            ],
            "x_mitre_shortname": "resource-development",
            "type": "x-mitre-tactic",
            "id": "x-mitre-tactic--8fb30c8b-43fa-486b-b652-5bedc52328da",
            "created": "2022-03-30T12:50:48.721Z",
            "modified": "2022-03-30T12:50:48.721Z"
        },
        {
            "name": "Initial Access",
            "description": "The adversary is trying to gain access to the system containing machine learning artifacts.\n\nThe target system could be a network, mobile device, or an edge device such as a sensor platform.\nThe machine learning capabilities used by the system could be local with onboard or cloud enabled ML capabilities.\n\nInitial Access consists of techniques that use various entry vectors to gain their initial foothold within the system.\n",
            "external_references": [
                {
                    "source_name": "mitre-atlas",
                    "url": "https://localhost:4200/tactics/AML.TA0004",
                    "external_id": "AML.TA0004"
                }
            ],
            "x_mitre_shortname": "initial-access",
            "type": "x-mitre-tactic",
            "id": "x-mitre-tactic--350b0b74-19a6-4383-806e-f643f60fc829",
            "created": "2022-03-30T12:50:48.721Z",
            "modified": "2022-03-30T12:50:48.721Z"
        },
        {
            "name": "ML Model Access",
            "description": "An adversary is attempting to gain some level of access to a machine learning model.\n\nML Model Access consists of techniques that use various types of access to the machine learning model that can be used by the adversary to gain information, develop attacks, and as a means to input data to the model.\nThe level of access can range from the full knowledge of the internals of the model to access to the physical environment where data is collected for use in the machine learning model.\nThe adversary may use varying levels of model access during the course of their attack, from staging the attack to impacting the target system.\n",
            "external_references": [
                {
                    "source_name": "mitre-atlas",
                    "url": "https://localhost:4200/tactics/AML.TA0000",
                    "external_id": "AML.TA0000"
                }
            ],
            "x_mitre_shortname": "ml-model-access",
            "type": "x-mitre-tactic",
            "id": "x-mitre-tactic--84e46311-9a2e-4a4c-a6ea-299e2f745152",
            "created": "2022-03-30T12:50:48.721Z",
            "modified": "2022-03-30T12:50:48.721Z"
        },
        {
            "name": "Execution",
            "description": "The adversary is trying to run malicious code.\n\nExecution consists of techniques that result in adversary-controlled code running on a local or remote system.\nTechniques that run malicious code are often paired with techniques from all other tactics to achieve broader goals, like exploring a network or stealing data.\nFor example, an adversary might use a remote access tool to run a PowerShell script that does Remote System Discovery.\n",
            "external_references": [
                {
                    "source_name": "mitre-atlas",
                    "url": "https://localhost:4200/tactics/AML.TA0005",
                    "external_id": "AML.TA0005"
                }
            ],
            "x_mitre_shortname": "execution",
            "type": "x-mitre-tactic",
            "id": "x-mitre-tactic--e6a7247f-3bd7-4c5e-be38-e57bca3fb7e3",
            "created": "2022-03-30T12:50:48.722Z",
            "modified": "2022-03-30T12:50:48.722Z"
        },
        {
            "name": "Persistence",
            "description": "The adversary is trying to maintain their foothold.\n\nPersistence consists of techniques that adversaries use to keep access to systems across restarts, changed credentials, and other interruptions that could cut off their access.\nTechniques used for persistence often involve leaving behind modified ML artifacts such as poisoned training data or backdoored ML models.\n",
            "external_references": [
                {
                    "source_name": "mitre-atlas",
                    "url": "https://localhost:4200/tactics/AML.TA0006",
                    "external_id": "AML.TA0006"
                }
            ],
            "x_mitre_shortname": "persistence",
            "type": "x-mitre-tactic",
            "id": "x-mitre-tactic--6b7f2588-7009-4462-a77a-7a7df7d4551a",
            "created": "2022-03-30T12:50:48.722Z",
            "modified": "2022-03-30T12:50:48.722Z"
        },
        {
            "name": "Defense Evasion",
            "description": "The adversary is trying to avoid being detected by security software.\n\nDefense Evasion consists of techniques that adversaries use to avoid detection throughout their compromise.\nTechniques used for defense evasion include evading ML-enabled security software such as malware detectors.\n",
            "external_references": [
                {
                    "source_name": "mitre-atlas",
                    "url": "https://localhost:4200/tactics/AML.TA0007",
                    "external_id": "AML.TA0007"
                }
            ],
            "x_mitre_shortname": "defense-evasion",
            "type": "x-mitre-tactic",
            "id": "x-mitre-tactic--9a10021e-3678-4fde-a484-e415ee46ca90",
            "created": "2022-03-30T12:50:48.722Z",
            "modified": "2022-03-30T12:50:48.722Z"
        },
        {
            "name": "Discovery",
            "description": "The adversary is trying to figure out your environment.\n\nDiscovery consists of techniques an adversary may use to gain knowledge about the system and internal network.\nThese techniques help adversaries observe the environment and orient themselves before deciding how to act.\nThey also allow adversaries to explore what they can control and what's around their entry point in order to discover how it could benefit their current objective.\nNative operating system tools are often used toward this post-compromise information-gathering objective.\n",
            "external_references": [
                {
                    "source_name": "mitre-atlas",
                    "url": "https://localhost:4200/tactics/AML.TA0008",
                    "external_id": "AML.TA0008"
                }
            ],
            "x_mitre_shortname": "discovery",
            "type": "x-mitre-tactic",
            "id": "x-mitre-tactic--324dbcf8-660d-4a31-9d66-e068e09338ac",
            "created": "2022-03-30T12:50:48.722Z",
            "modified": "2022-03-30T12:50:48.722Z"
        },
        {
            "name": "Collection",
            "description": "The adversary is trying to gather ML artifacts and other related information relevant to their goal.\n\nCollection consists of techniques adversaries may use to gather information and the sources information is collected from that are relevant to following through on the adversary's objectives.\nFrequently, the next goal after collecting data is to steal (exfiltrate) the ML artifacts, or use the collected information to stage future operations.\nCommon target sources include software repositories, container registries, model repositories, and object stores.\n",
            "external_references": [
                {
                    "source_name": "mitre-atlas",
                    "url": "https://localhost:4200/tactics/AML.TA0009",
                    "external_id": "AML.TA0009"
                }
            ],
            "x_mitre_shortname": "collection",
            "type": "x-mitre-tactic",
            "id": "x-mitre-tactic--55d3cb22-4581-46b2-82e4-a457a02e208a",
            "created": "2022-03-30T12:50:48.722Z",
            "modified": "2022-03-30T12:50:48.722Z"
        },
        {
            "name": "ML Attack Staging",
            "description": "An adversary is leveraging their knowledge of and access to the target system to tailor the attack.\n\nML Attack Staging consists of techniques adversaries use to prepare their attack on the target ML model.\nTechniques can include training proxy models, poisoning the target model, and crafting adversarial data to feed the target model.\nSome of these techniques can be performed in an offline manor and are thus difficult to mitigate.\nThese techniques are often used to achieve the adversary's end goal.\n",
            "external_references": [
                {
                    "source_name": "mitre-atlas",
                    "url": "https://localhost:4200/tactics/AML.TA0001",
                    "external_id": "AML.TA0001"
                }
            ],
            "x_mitre_shortname": "ml-attack-staging",
            "type": "x-mitre-tactic",
            "id": "x-mitre-tactic--0e121e2a-2964-41b6-8ea7-2c3df266e816",
            "created": "2022-03-30T12:50:48.722Z",
            "modified": "2022-03-30T12:50:48.722Z"
        },
        {
            "name": "Exfiltration",
            "description": "The adversary is trying to steal machine learning artifacts.\n\nExfiltration consists of techniques that adversaries may use to steal data from your network.\nData may be stolen for it's valuable intellectual property, or for use in staging future operations.\n\nTechniques for getting data out of a target network typically include transferring it over their command and control channel or an alternate channel and may also include putting size limits on the transmission.\n",
            "external_references": [
                {
                    "source_name": "mitre-atlas",
                    "url": "https://localhost:4200/tactics/AML.TA0010",
                    "external_id": "AML.TA0010"
                }
            ],
            "x_mitre_shortname": "exfiltration",
            "type": "x-mitre-tactic",
            "id": "x-mitre-tactic--d1da7668-82eb-4a66-a7e1-ce3db73ab6b4",
            "created": "2022-03-30T12:50:48.722Z",
            "modified": "2022-03-30T12:50:48.722Z"
        },
        {
            "name": "Impact",
            "description": "The adversary is trying to manipulate, interrupt, erode confidence in, or destroy your systems and data.\n\nImpact consists of techniques that adversaries use to disrupt availability or compromise integrity by manipulating business and operational processes.\nTechniques used for impact can include destroying or tampering with data.\nIn some cases, business processes can look fine, but may have been altered to benefit the adversaries' goals.\nThese techniques might be used by adversaries to follow through on their end goal or to provide cover for a confidentiality breach.\n",
            "external_references": [
                {
                    "source_name": "mitre-atlas",
                    "url": "https://localhost:4200/tactics/AML.TA0011",
                    "external_id": "AML.TA0011"
                }
            ],
            "x_mitre_shortname": "impact",
            "type": "x-mitre-tactic",
            "id": "x-mitre-tactic--00d566f1-21f0-4d27-b45b-7aa74b9fbd7b",
            "created": "2022-03-30T12:50:48.722Z",
            "modified": "2022-03-30T12:50:48.722Z"
        },
        {
            "name": "Security Controls",
            "description": "Alter security controls to make the system more or less vulnerable to attack.\n\nManipulating Security Controls involves making configuration changes to a system's security settings including modifying Group Policies, disabling/enabling autorun for removable media, tightening or relaxing system firewalls, etc. Such security controls can be tightened to dissuade or prevent adversary activity. Conversely, security controls can be weakened or left overly permissive to encourage or enable adversary activity. \\n<br><br>\\nTightening security controls can typically be done by implementing any of the mitigations described in MITRE ATT&CK. See https://attack.mitre.org/mitigations/enterprise/ for a full list of mitigation strategies. While loosening security controls may seem obvious (i.e., simply don't employ a given mitigation strategy), there is an additional level of nuance that must be considered\n",
            "kill_chain_phases": [
                {
                    "kill_chain_name": "mitre-atlas",
                    "phase_name": "reconnaissance"
                }
            ],
            "external_references": [
                {
                    "source_name": "mitre-atlas",
                    "url": "https://localhost:4200/techniques/EAC0018",
                    "external_id": "EAC0018"
                }
            ],
            "x_mitre_platforms": [
                "ATLAS"
            ],
            "type": "attack-pattern",
            "id": "attack-pattern--124d09af-3162-48c5-9bf9-bebe3d5d9bf4",
            "created": "2022-03-30T12:50:48.706Z",
            "modified": "2022-03-30T12:50:48.706Z"
        },
        {
            "name": "Search for Victim's Publicly Available Research Materials",
            "description": "Adversaries may search publicly available research to learn how and where machine learning is used within a victim organization.\nThe adversary can use this information to identify targets for attack, or to tailor an existing attack to make it more effective.\nOrganizations often use open source model architectures trained on additional proprietary data in production.\nKnowledge of this underlying architecture allows the adversary to craft more realistic proxy models ([Create Proxy ML Model](/techniques/AML.T0005)).\nAn adversary can search these resources for publications for authors employed at the victim organization.\n\nResearch materials may exist as academic papers published in [Journals and Conference Proceedings](/techniques/AML.T0000.000), or stored in [Pre-Print Repositories](/techniques/AML.T0000.001), as well as [Technical Blogs](/techniques/AML.T0000.002).\n",
            "kill_chain_phases": [
                {
                    "kill_chain_name": "mitre-atlas",
                    "phase_name": "reconnaissance"
                }
            ],
            "external_references": [
                {
                    "source_name": "mitre-atlas",
                    "url": "https://localhost:4200/techniques/AML.T0000",
                    "external_id": "AML.T0000"
                }
            ],
            "x_mitre_platforms": [
                "ATLAS"
            ],
            "type": "attack-pattern",
            "id": "attack-pattern--a80307ac-eb6b-496f-8fe1-b36e6b83caf2",
            "created": "2022-03-30T12:50:48.707Z",
            "modified": "2022-03-30T12:50:48.707Z"
        },
        {
            "name": "Journals and Conference Proceedings",
            "description": "Many of the publications accepted at premier machine learning conferences and journals come from commercial labs.\nSome journals and conferences are open access, others may require paying for access or a membership.\nThese publications will often describe in detail all aspects of a particular approach for reproducibility.\nThis information can be used by adversaries to implement the paper.\n",
            "kill_chain_phases": [
                {
                    "kill_chain_name": "mitre-atlas",
                    "phase_name": "reconnaissance"
                }
            ],
            "external_references": [
                {
                    "source_name": "mitre-atlas",
                    "url": "https://localhost:4200/techniques/AML.T0000.000",
                    "external_id": "AML.T0000.000"
                }
            ],
            "x_mitre_platforms": [
                "ATLAS"
            ],
            "x_mitre_is_subtechnique": true,
            "type": "attack-pattern",
            "id": "attack-pattern--38e5993d-7751-4dd3-a88e-d5bcb6ca105d",
            "created": "2022-03-30T12:50:48.707Z",
            "modified": "2022-03-30T12:50:48.707Z"
        },
        {
            "name": "Pre-Print Repositories",
            "description": "Pre-Print repositories, such as arXiv, contain the latest academic research papers that haven't been peer reviewed.\nThey may contain research notes, or technical reports that aren't typically published in journals or conference proceedings.\nPre-print repositories also serve as a central location to share papers that have been accepted to journals.\nSearching pre-print repositories  provide adversaries with a relatively up-to-date view of what researchers in the victim organization are working on.\n",
            "kill_chain_phases": [
                {
                    "kill_chain_name": "mitre-atlas",
                    "phase_name": "reconnaissance"
                }
            ],
            "external_references": [
                {
                    "source_name": "mitre-atlas",
                    "url": "https://localhost:4200/techniques/AML.T0000.001",
                    "external_id": "AML.T0000.001"
                }
            ],
            "x_mitre_platforms": [
                "ATLAS"
            ],
            "x_mitre_is_subtechnique": true,
            "type": "attack-pattern",
            "id": "attack-pattern--4bf5671b-c3fd-4170-a949-69c1c14276c7",
            "created": "2022-03-30T12:50:48.708Z",
            "modified": "2022-03-30T12:50:48.708Z"
        },
        {
            "name": "Technical Blogs",
            "description": "Research labs at academic institutions and Company R&D divisions often have blogs that highlight their use of machine learning and its application to the organizations unique problems.\nIndividual researchers also frequently document their work in blogposts.\nAn adversary may search for posts made by the target victim organization or its employees.\nIn comparison to [Journals and Conference Proceedings](/techniques/AML.T0000.000) and [Pre-Print Repositories](/techniques/AML.T0000.001) this material will often contain more practical aspects of the machine learning system.\nThis could include underlying technologies and frameworks used, and possibly some information about the API access and use case.\nThis will help the adversary better understand how that organization is using machine learning internally and the details of their approach that could aid in tailoring an attack.\n",
            "kill_chain_phases": [
                {
                    "kill_chain_name": "mitre-atlas",
                    "phase_name": "reconnaissance"
                }
            ],
            "external_references": [
                {
                    "source_name": "mitre-atlas",
                    "url": "https://localhost:4200/techniques/AML.T0000.002",
                    "external_id": "AML.T0000.002"
                }
            ],
            "x_mitre_platforms": [
                "ATLAS"
            ],
            "x_mitre_is_subtechnique": true,
            "type": "attack-pattern",
            "id": "attack-pattern--5d522173-f322-433e-bfca-694cd739c30b",
            "created": "2022-03-30T12:50:48.708Z",
            "modified": "2022-03-30T12:50:48.708Z"
        },
        {
            "name": "Search for Publicly Available Adversarial Vulnerability Analysis",
            "description": "Much like the [Search for Victim's Publicly Available Research Materials](/techniques/AML.T0000), there is often ample research available on the vulnerabilities of common models. Once a target has been identified, an adversary will likely try to identify any pre-existing work that has been done for this class of models.\nThis will include not only reading academic papers that may identify the particulars of a successful attack, but also identifying pre-existing implementations of those attacks. The adversary may [Adversarial ML Attack Implementations](/techniques/AML.T0016.000) or [Develop Adversarial ML Attack Capabilities](/techniques/AML.T0017) their own if necessary.",
            "kill_chain_phases": [
                {
                    "kill_chain_name": "mitre-atlas",
                    "phase_name": "reconnaissance"
                }
            ],
            "external_references": [
                {
                    "source_name": "mitre-atlas",
                    "url": "https://localhost:4200/techniques/AML.T0001",
                    "external_id": "AML.T0001"
                }
            ],
            "x_mitre_platforms": [
                "ATLAS"
            ],
            "type": "attack-pattern",
            "id": "attack-pattern--47ccc726-5c46-4208-8325-4e00c332af26",
            "created": "2022-03-30T12:50:48.708Z",
            "modified": "2022-03-30T12:50:48.708Z"
        },
        {
            "name": "Search Victim-Owned Websites",
            "description": "Adversaries may search websites owned by the victim for information that can be used during targeting.\nVictim-owned websites may contain technical details about their ML-enabled products or services.\nVictim-owned websites may contain a variety of details, including names of departments/divisions, physical locations, and data about key employees such as names, roles, and contact info.\nThese sites may also have details highlighting business operations and relationships.\n\nAdversaries may search victim-owned websites to gather actionable information.\nThis information may help adversaries tailor their attacks (e.g. [Develop Adversarial ML Attack Capabilities](/techniques/AML.T0017) or [Manual Modification](/techniques/AML.T0043.003)).\nInformation from these sources may reveal opportunities for other forms of reconnaissance (e.g. [Search for Victim's Publicly Available Research Materials](/techniques/AML.T0000) or [Search for Publicly Available Adversarial Vulnerability Analysis](/techniques/AML.T0001))\n",
            "kill_chain_phases": [
                {
                    "kill_chain_name": "mitre-atlas",
                    "phase_name": "reconnaissance"
                }
            ],
            "external_references": [
                {
                    "source_name": "mitre-atlas",
                    "url": "https://localhost:4200/techniques/AML.T0003",
                    "external_id": "AML.T0003"
                }
            ],
            "x_mitre_platforms": [
                "ATLAS"
            ],
            "type": "attack-pattern",
            "id": "attack-pattern--c8661510-f27d-4d7e-a694-3c6082ac0f68",
            "created": "2022-03-30T12:50:48.708Z",
            "modified": "2022-03-30T12:50:48.708Z"
        },
        {
            "name": "Search Application Repositories",
            "description": "Adversaries may search open application repositories during targeting.\nExamples of these include Google Play, the iOS App store, the macOS App Store, and the Microsoft Store.\n\nAdversaries may craft search queries seeking applications that contain a ML-enabled components.\nFrequently, the next step is to [Acquire Public ML Artifacts](/techniques/AML.T0002).\n",
            "kill_chain_phases": [
                {
                    "kill_chain_name": "mitre-atlas",
                    "phase_name": "reconnaissance"
                }
            ],
            "external_references": [
                {
                    "source_name": "mitre-atlas",
                    "url": "https://localhost:4200/techniques/AML.T0004",
                    "external_id": "AML.T0004"
                }
            ],
            "x_mitre_platforms": [
                "ATLAS"
            ],
            "type": "attack-pattern",
            "id": "attack-pattern--a36412ef-1361-4b9b-b217-d3ddb2593b37",
            "created": "2022-03-30T12:50:48.709Z",
            "modified": "2022-03-30T12:50:48.709Z"
        },
        {
            "name": "Active Scanning",
            "description": "An adversary may probe or scan the victim system to gather information for targeting.\nThis is distinct from other reconnaissance techniques that do not involve direct interaction with the victim system.\n",
            "kill_chain_phases": [
                {
                    "kill_chain_name": "mitre-atlas",
                    "phase_name": "reconnaissance"
                }
            ],
            "external_references": [
                {
                    "source_name": "mitre-atlas",
                    "url": "https://localhost:4200/techniques/AML.T0006",
                    "external_id": "AML.T0006"
                }
            ],
            "x_mitre_platforms": [
                "ATLAS"
            ],
            "type": "attack-pattern",
            "id": "attack-pattern--45bc6609-a022-4542-9fbf-ea762c58d351",
            "created": "2022-03-30T12:50:48.709Z",
            "modified": "2022-03-30T12:50:48.709Z"
        },
        {
            "name": "Acquire Public ML Artifacts",
            "description": "Adversaries may search public sources, including cloud storage, public-facing services, and software or data repositories, to identify machine learning artifacts.\nThese machine learning artifacts may include the software stack used to train and deploy models, training and testing data, model configurations and parameters.\nAn adversary will be particularly interested in artifacts hosted by or associated with the victim organization as they may represent what that organization uses in a production environment.\nAdversaries may identify artifact repositories via other resources associated with the victim organization (e.g. [Search Victim-Owned Websites](/techniques/AML.T0003) or [Search for Victim's Publicly Available Research Materials](/techniques/AML.T0000)).\nThese ML artifacts often provide adversaries with details of the ML task and approach.\n\nML artifacts can aid in an adversary's ability to [Create Proxy ML Model](/techniques/AML.T0005).\nIf these artifacts include pieces of the actual model in production, they can be used to directly [Craft Adversarial Data](/techniques/AML.T0043).\nAcquiring some artifacts requires registration (providing user details such email/name), AWS keys, or written requests, and may require the adversary to [Establish Accounts](/techniques/AML.T0021).\n\nArtifacts might be hosted on victim-controlled infrastructure, providing the victim with some information on who has accessed that data.\n",
            "kill_chain_phases": [
                {
                    "kill_chain_name": "mitre-atlas",
                    "phase_name": "resource-development"
                }
            ],
            "external_references": [
                {
                    "source_name": "mitre-atlas",
                    "url": "https://localhost:4200/techniques/AML.T0002",
                    "external_id": "AML.T0002"
                }
            ],
            "x_mitre_platforms": [
                "ATLAS"
            ],
            "type": "attack-pattern",
            "id": "attack-pattern--56c291a6-c822-402c-bec2-10813ba60027",
            "created": "2022-03-30T12:50:48.709Z",
            "modified": "2022-03-30T12:50:48.709Z"
        },
        {
            "name": "Datasets",
            "description": "Adversaries may collect public datasets to use in their operations.\nDatasets used by the victim organization or datasets that are representative of the data used by the victim organization may be valuable to adversaries.\nDatasets can be stored in cloud storage, or on victim-owned websites.\nSome datasets require the adversary to [Establish Accounts](/techniques/AML.T0021) for access.\n\nAcquired datasets help the adversary advance their operations, stage attacks,  and tailor attacks to the victim organization.\n",
            "kill_chain_phases": [
                {
                    "kill_chain_name": "mitre-atlas",
                    "phase_name": "resource-development"
                }
            ],
            "external_references": [
                {
                    "source_name": "mitre-atlas",
                    "url": "https://localhost:4200/techniques/AML.T0002.000",
                    "external_id": "AML.T0002.000"
                }
            ],
            "x_mitre_platforms": [
                "ATLAS"
            ],
            "x_mitre_is_subtechnique": true,
            "type": "attack-pattern",
            "id": "attack-pattern--3bcbf550-0e37-495d-8099-91c1f07095d7",
            "created": "2022-03-30T12:50:48.709Z",
            "modified": "2022-03-30T12:50:48.709Z"
        },
        {
            "name": "Models",
            "description": "Adversaries may acquire public models to use in their operations.\nAdversaries may seek models used by the victim organization or models that are representative of those used by the victim organization.\nRepresentative models may include model architectures, or pre-trained models which define the architecture as well as model parameters from training on a dataset.\nThe adversary may search public sources for common model architecture configuration file formats such as yaml or python configuration files, and common model storage file formats such as ONNX (.onnx), HDF5 (.h5), Pickle (.pkl), PyTorch (.pth), or TensorFlow (.pb, .tflite).\n\nAcquired models are useful in advancing the adversary's operations and are frequently used to tailor attacks to the victim model.\n",
            "kill_chain_phases": [
                {
                    "kill_chain_name": "mitre-atlas",
                    "phase_name": "resource-development"
                }
            ],
            "external_references": [
                {
                    "source_name": "mitre-atlas",
                    "url": "https://localhost:4200/techniques/AML.T0002.001",
                    "external_id": "AML.T0002.001"
                }
            ],
            "x_mitre_platforms": [
                "ATLAS"
            ],
            "x_mitre_is_subtechnique": true,
            "type": "attack-pattern",
            "id": "attack-pattern--d7253fda-7e4e-40aa-99a0-98f30d0d8312",
            "created": "2022-03-30T12:50:48.709Z",
            "modified": "2022-03-30T12:50:48.709Z"
        },
        {
            "name": "Obtain Capabilities",
            "description": "Adversaries may search for and obtain software capabilities for use in their operations.\nCapabilities may be specific to ML-based attacks [Adversarial ML Attack Implementations](/techniques/AML.T0016.000) or generic software tools repurposed for malicious intent ([Software Tools](/techniques/AML.T0016.001)). In both instances, an adversary may modify or customize the capability to aid in targeting a particular ML system.",
            "kill_chain_phases": [
                {
                    "kill_chain_name": "mitre-atlas",
                    "phase_name": "resource-development"
                }
            ],
            "external_references": [
                {
                    "source_name": "mitre-atlas",
                    "url": "https://localhost:4200/techniques/AML.T0016",
                    "external_id": "AML.T0016"
                }
            ],
            "x_mitre_platforms": [
                "ATLAS"
            ],
            "type": "attack-pattern",
            "id": "attack-pattern--2ca11647-62f6-4a2a-b4cf-2c7aa90a3308",
            "created": "2022-03-30T12:50:48.710Z",
            "modified": "2022-03-30T12:50:48.710Z"
        },
        {
            "name": "Adversarial ML Attack Implementations",
            "description": "Adversaries may search for existing open source implementations of machine learning attacks. The research community often publishes their code for reproducibility and to further future research. Libraries intended for research purposes, such as CleverHans, the Adversarial Robustness Toolbox, and FoolBox, can be weaponized by an adversary. Adversaries may also obtain and use tools that were not originally designed for adversarial ML attacks as part of their attack.",
            "kill_chain_phases": [
                {
                    "kill_chain_name": "mitre-atlas",
                    "phase_name": "resource-development"
                }
            ],
            "external_references": [
                {
                    "source_name": "mitre-atlas",
                    "url": "https://localhost:4200/techniques/AML.T0016.000",
                    "external_id": "AML.T0016.000"
                }
            ],
            "x_mitre_platforms": [
                "ATLAS"
            ],
            "x_mitre_is_subtechnique": true,
            "type": "attack-pattern",
            "id": "attack-pattern--736a7a7e-d768-4434-9ae9-b3ce83c3a7c6",
            "created": "2022-03-30T12:50:48.710Z",
            "modified": "2022-03-30T12:50:48.710Z"
        },
        {
            "name": "Software Tools",
            "description": "Adversaries may search for and obtain software tools to support their operations. Software designed for legitimate use may be repurposed by an adversary for malicious intent. An adversary may modify or customize software tools to achieve their purpose. Software tools used to support attacks on ML systems are not necessarily ML-based themselves.\n",
            "kill_chain_phases": [
                {
                    "kill_chain_name": "mitre-atlas",
                    "phase_name": "resource-development"
                }
            ],
            "external_references": [
                {
                    "source_name": "mitre-atlas",
                    "url": "https://localhost:4200/techniques/AML.T0016.001",
                    "external_id": "AML.T0016.001"
                }
            ],
            "x_mitre_platforms": [
                "ATLAS"
            ],
            "x_mitre_is_subtechnique": true,
            "type": "attack-pattern",
            "id": "attack-pattern--882e2152-ea87-4bac-94bb-9a4ff39220ed",
            "created": "2022-03-30T12:50:48.710Z",
            "modified": "2022-03-30T12:50:48.710Z"
        },
        {
            "name": "Develop Adversarial ML Attack Capabilities",
            "description": "Adversaries may develop their own adversarial attacks. They may leverage existing libraries as a starting point ([Adversarial ML Attack Implementations](/techniques/AML.T0016.000)). They may implement ideas described in public research papers or develop custom made attacks for the victim model.",
            "kill_chain_phases": [
                {
                    "kill_chain_name": "mitre-atlas",
                    "phase_name": "resource-development"
                }
            ],
            "external_references": [
                {
                    "source_name": "mitre-atlas",
                    "url": "https://localhost:4200/techniques/AML.T0017",
                    "external_id": "AML.T0017"
                }
            ],
            "x_mitre_platforms": [
                "ATLAS"
            ],
            "type": "attack-pattern",
            "id": "attack-pattern--316d863f-d462-4b88-b82d-4b364c29f667",
            "created": "2022-03-30T12:50:48.710Z",
            "modified": "2022-03-30T12:50:48.710Z"
        },
        {
            "name": "Acquire Infrastructure",
            "description": "Adversaries may buy, lease, or rent infrastructure for use throughout their operation.\nA wide variety of infrastructure exists for hosting and orchestrating adversary operations.\nInfrastructure solutions include physical or cloud servers, domains, mobile devices, and third-party web services.\nFree resources may also be used, but they are typically limited.\n\nUse of these infrastructure solutions allows an adversary to stage, launch, and execute an operation.\nSolutions may help adversary operations blend in with traffic that is seen as normal, such as contact to third-party web services.\nDepending on the implementation, adversaries may use infrastructure that makes it difficult to physically tie back to them as well as utilize infrastructure that can be rapidly provisioned, modified, and shut down.\n",
            "kill_chain_phases": [
                {
                    "kill_chain_name": "mitre-atlas",
                    "phase_name": "resource-development"
                }
            ],
            "external_references": [
                {
                    "source_name": "mitre-atlas",
                    "url": "https://localhost:4200/techniques/AML.T0008",
                    "external_id": "AML.T0008"
                }
            ],
            "x_mitre_platforms": [
                "ATLAS"
            ],
            "type": "attack-pattern",
            "id": "attack-pattern--f509b9d5-c2cf-4419-8e18-90e9183381a7",
            "created": "2022-03-30T12:50:48.710Z",
            "modified": "2022-03-30T12:50:48.710Z"
        },
        {
            "name": "ML Development Workspaces",
            "description": "Developing and staging machine learning attacks often requires expensive compute resources.\nAdversaries may need access to one or many GPUs in order to develop an attack.\nThey may try to anonymously use free resources such as Google Colaboratory, or cloud resources such as AWS, Azure, or Google Cloud as an efficient way to stand up temporary resources to conduct operations.\nMultiple workspaces may be used to avoid detection.\n",
            "kill_chain_phases": [
                {
                    "kill_chain_name": "mitre-atlas",
                    "phase_name": "resource-development"
                }
            ],
            "external_references": [
                {
                    "source_name": "mitre-atlas",
                    "url": "https://localhost:4200/techniques/AML.T0008.000",
                    "external_id": "AML.T0008.000"
                }
            ],
            "x_mitre_platforms": [
                "ATLAS"
            ],
            "x_mitre_is_subtechnique": true,
            "type": "attack-pattern",
            "id": "attack-pattern--58c08543-0c31-4eee-b786-036086f8e2db",
            "created": "2022-03-30T12:50:48.710Z",
            "modified": "2022-03-30T12:50:48.710Z"
        },
        {
            "name": "Consumer Hardware",
            "description": "Adversaries may acquire consumer hardware to conduct their attacks.\nOwning the hardware provides the adversary with complete control of the environment. These devices can be hard to trace.\n",
            "kill_chain_phases": [
                {
                    "kill_chain_name": "mitre-atlas",
                    "phase_name": "resource-development"
                }
            ],
            "external_references": [
                {
                    "source_name": "mitre-atlas",
                    "url": "https://localhost:4200/techniques/AML.T0008.001",
                    "external_id": "AML.T0008.001"
                }
            ],
            "x_mitre_platforms": [
                "ATLAS"
            ],
            "x_mitre_is_subtechnique": true,
            "type": "attack-pattern",
            "id": "attack-pattern--3e0bb768-3661-4aa4-9704-c4db1b1d24b3",
            "created": "2022-03-30T12:50:48.711Z",
            "modified": "2022-03-30T12:50:48.711Z"
        },
        {
            "name": "Publish Poisoned Datasets",
            "description": "Adversaries may [Poison Training Data](/techniques/AML.T0020) and publish it to a public location.\nThe poisoned dataset may be a novel dataset or a poisoned variant of an existing open source dataset.\nThis data may be introduced to a victim system via [ML Supply Chain Compromise](/techniques/AML.T0010).\n",
            "kill_chain_phases": [
                {
                    "kill_chain_name": "mitre-atlas",
                    "phase_name": "resource-development"
                }
            ],
            "external_references": [
                {
                    "source_name": "mitre-atlas",
                    "url": "https://localhost:4200/techniques/AML.T0019",
                    "external_id": "AML.T0019"
                }
            ],
            "x_mitre_platforms": [
                "ATLAS"
            ],
            "type": "attack-pattern",
            "id": "attack-pattern--746a9d48-3006-4780-94fe-6ec4ca6a8ace",
            "created": "2022-03-30T12:50:48.711Z",
            "modified": "2022-03-30T12:50:48.711Z"
        },
        {
            "name": "ML Supply Chain Compromise",
            "description": "Adversaries may gain initial access to a system by compromising the unique portions of the ML supply chain.\nThis could include [GPU Hardware](/techniques/AML.T0010.000), [Data](/techniques/AML.T0010.002) and its annotations, parts of the ML [ML Software](/techniques/AML.T0010.001) stack, or the [Model](/techniques/AML.T0010.003) itself.\nIn some instances the attacker will need secondary access to fully carry out an attack using compromised components of the supply chain.\n",
            "kill_chain_phases": [
                {
                    "kill_chain_name": "mitre-atlas",
                    "phase_name": "initial-access"
                }
            ],
            "external_references": [
                {
                    "source_name": "mitre-atlas",
                    "url": "https://localhost:4200/techniques/AML.T0010",
                    "external_id": "AML.T0010"
                }
            ],
            "x_mitre_platforms": [
                "ATLAS"
            ],
            "type": "attack-pattern",
            "id": "attack-pattern--b22089b2-88ff-4ac7-bd09-8585e6897e8a",
            "created": "2022-03-30T12:50:48.711Z",
            "modified": "2022-03-30T12:50:48.711Z"
        },
        {
            "name": "GPU Hardware",
            "description": "Most machine learning systems require access to certain specialized hardware, typically GPUs.\nAdversaries can target machine learning systems by specifically targeting the GPU supply chain.\n",
            "kill_chain_phases": [
                {
                    "kill_chain_name": "mitre-atlas",
                    "phase_name": "initial-access"
                }
            ],
            "external_references": [
                {
                    "source_name": "mitre-atlas",
                    "url": "https://localhost:4200/techniques/AML.T0010.000",
                    "external_id": "AML.T0010.000"
                }
            ],
            "x_mitre_platforms": [
                "ATLAS"
            ],
            "x_mitre_is_subtechnique": true,
            "type": "attack-pattern",
            "id": "attack-pattern--db9ddb59-d7e2-4fbd-b593-68a8fd0808ac",
            "created": "2022-03-30T12:50:48.711Z",
            "modified": "2022-03-30T12:50:48.711Z"
        },
        {
            "name": "ML Software",
            "description": "Most machine learning systems rely on a limited set of machine learning frameworks.\nAn adversary could get access to a large number of machine learning systems through a comprise of one of their supply chains.\nMany machine learning projects also rely on other open source implementations of various algorithms.\nThese can also be compromised in a targeted way to get access to specific systems.\n",
            "kill_chain_phases": [
                {
                    "kill_chain_name": "mitre-atlas",
                    "phase_name": "initial-access"
                }
            ],
            "external_references": [
                {
                    "source_name": "mitre-atlas",
                    "url": "https://localhost:4200/techniques/AML.T0010.001",
                    "external_id": "AML.T0010.001"
                }
            ],
            "x_mitre_platforms": [
                "ATLAS"
            ],
            "x_mitre_is_subtechnique": true,
            "type": "attack-pattern",
            "id": "attack-pattern--5c140d2d-193b-400a-beab-79fe8b2ee0ba",
            "created": "2022-03-30T12:50:48.711Z",
            "modified": "2022-03-30T12:50:48.711Z"
        },
        {
            "name": "Data",
            "description": "Data is a key vector of supply chain compromise for adversaries.\nEvery machine learning project will require some form of data.\nMany rely on large open source datasets that are publicly available.\nAn adversary could rely on compromising these sources of data.\nThe malicious data could be a result of [Poison Training Data](/techniques/AML.T0020) or include traditional malware.\n\nAn adversary can also target private datasets in the labeling phase.\nThe creation of private datasets will often require the hiring of outside labeling services.\nAn adversary can poison a dataset by modifying the labels being generated by the labeling service.\n",
            "kill_chain_phases": [
                {
                    "kill_chain_name": "mitre-atlas",
                    "phase_name": "initial-access"
                }
            ],
            "external_references": [
                {
                    "source_name": "mitre-atlas",
                    "url": "https://localhost:4200/techniques/AML.T0010.002",
                    "external_id": "AML.T0010.002"
                }
            ],
            "x_mitre_platforms": [
                "ATLAS"
            ],
            "x_mitre_is_subtechnique": true,
            "type": "attack-pattern",
            "id": "attack-pattern--14224938-47f3-4ddb-a865-d6e3417e4bf1",
            "created": "2022-03-30T12:50:48.712Z",
            "modified": "2022-03-30T12:50:48.712Z"
        },
        {
            "name": "Model",
            "description": "Machine learning systems often rely on open sourced models in various ways.\nMost commonly, the victim organization may be using these models for fine tuning.\nThese models will be downloaded from an external source and then used as the base for the model as it is tuned on a smaller, private dataset.\nLoading models often requires executing some saved code in the form of a saved model file.\nThese can be compromised with traditional malware, or through some adversarial machine learning techniques.\n",
            "kill_chain_phases": [
                {
                    "kill_chain_name": "mitre-atlas",
                    "phase_name": "initial-access"
                }
            ],
            "external_references": [
                {
                    "source_name": "mitre-atlas",
                    "url": "https://localhost:4200/techniques/AML.T0010.003",
                    "external_id": "AML.T0010.003"
                }
            ],
            "x_mitre_platforms": [
                "ATLAS"
            ],
            "x_mitre_is_subtechnique": true,
            "type": "attack-pattern",
            "id": "attack-pattern--d854be46-0ae0-4744-a715-7ac593dcb618",
            "created": "2022-03-30T12:50:48.712Z",
            "modified": "2022-03-30T12:50:48.712Z"
        },
        {
            "name": "ML Model Inference API Access",
            "description": "Adversaries may gain access to a model via legitimate access to the inference API.\nInference API access can be a source of information to the adversary ([Discover ML Model Ontology](/techniques/AML.T0013), [Discover ML Model Family](/techniques/AML.T0013)), a means of staging the attack ([Verify Attack](/techniques/AML.T0042), [Craft Adversarial Data](/techniques/AML.T0043)), or for introducing data to the target system for Impact ([Evade ML Model](/techniques/AML.T0015), [Erode ML Model Integrity](/techniques/AML.T0031)).\n",
            "kill_chain_phases": [
                {
                    "kill_chain_name": "mitre-atlas",
                    "phase_name": "ml-model-access"
                }
            ],
            "external_references": [
                {
                    "source_name": "mitre-atlas",
                    "url": "https://localhost:4200/techniques/AML.T0040",
                    "external_id": "AML.T0040"
                }
            ],
            "x_mitre_platforms": [
                "ATLAS"
            ],
            "type": "attack-pattern",
            "id": "attack-pattern--c370871d-21b0-40c2-8064-41f66d0cc5d0",
            "created": "2022-03-30T12:50:48.712Z",
            "modified": "2022-03-30T12:50:48.712Z"
        },
        {
            "name": "ML-Enabled Product or Service",
            "description": "Adversaries may use a product or service that uses machine learning under the hood to gain access to the underlying machine learning model.\nThis type of indirect model access may reveal details of the ML model or its inferences in logs or metadata.\n",
            "kill_chain_phases": [
                {
                    "kill_chain_name": "mitre-atlas",
                    "phase_name": "ml-model-access"
                }
            ],
            "external_references": [
                {
                    "source_name": "mitre-atlas",
                    "url": "https://localhost:4200/techniques/AML.T0047",
                    "external_id": "AML.T0047"
                }
            ],
            "x_mitre_platforms": [
                "ATLAS"
            ],
            "type": "attack-pattern",
            "id": "attack-pattern--742b9be6-7865-480a-a5e5-edba93c9810e",
            "created": "2022-03-30T12:50:48.712Z",
            "modified": "2022-03-30T12:50:48.712Z"
        },
        {
            "name": "Physical Environment Access",
            "description": "In addition to the attacks that take place purely in the digital domain, adversaries may also exploit the physical environment for their attacks.\nIf the model is interacting with data collected from the real world in some way, the adversary can influence the model through access to wherever the data is being collected.\nBy modifying the data in the collection process, the adversary can perform modified versions of attacks designed for digital access.\n",
            "kill_chain_phases": [
                {
                    "kill_chain_name": "mitre-atlas",
                    "phase_name": "ml-model-access"
                }
            ],
            "external_references": [
                {
                    "source_name": "mitre-atlas",
                    "url": "https://localhost:4200/techniques/AML.T0041",
                    "external_id": "AML.T0041"
                }
            ],
            "x_mitre_platforms": [
                "ATLAS"
            ],
            "type": "attack-pattern",
            "id": "attack-pattern--a5ac037b-48ca-4fb5-a660-61bbab41851d",
            "created": "2022-03-30T12:50:48.712Z",
            "modified": "2022-03-30T12:50:48.712Z"
        },
        {
            "name": "Full ML Model Access",
            "description": "Adversaries may gain full \"white-box\" access to a machine learning model.\nThis means the adversary has complete knowledge of the model architecture, its parameters, and class ontology.\nThey may exfiltrate the model to [Craft Adversarial Data](/techniques/AML.T0043) and [Verify Attack](/techniques/AML.T0042) in an offline where it is hard to detect their behavior.\n",
            "kill_chain_phases": [
                {
                    "kill_chain_name": "mitre-atlas",
                    "phase_name": "ml-model-access"
                }
            ],
            "external_references": [
                {
                    "source_name": "mitre-atlas",
                    "url": "https://localhost:4200/techniques/AML.T0044",
                    "external_id": "AML.T0044"
                }
            ],
            "x_mitre_platforms": [
                "ATLAS"
            ],
            "type": "attack-pattern",
            "id": "attack-pattern--58e1e307-b497-45a6-bd28-eaf526d931d3",
            "created": "2022-03-30T12:50:48.712Z",
            "modified": "2022-03-30T12:50:48.712Z"
        },
        {
            "name": "Discover ML Model Ontology",
            "description": "Adversaries may discover the ontology of a machine learning model's output space, for example, the types of objects a model can detect.\nThe adversary may discovery the ontology by repeated queries to the model, forcing it to enumerate its output space.\nOr the ontology may be discovered in a configuration file or in documentation about the model.\n\nThe model ontology helps the adversary understand how the model is being used by the victim.\nIt is useful to the adversary in creating targeted attacks.\n",
            "kill_chain_phases": [
                {
                    "kill_chain_name": "mitre-atlas",
                    "phase_name": "discovery"
                }
            ],
            "external_references": [
                {
                    "source_name": "mitre-atlas",
                    "url": "https://localhost:4200/techniques/AML.T0013",
                    "external_id": "AML.T0013"
                }
            ],
            "x_mitre_platforms": [
                "ATLAS"
            ],
            "type": "attack-pattern",
            "id": "attack-pattern--49dcca72-9f8e-4a61-87aa-be406cda946e",
            "created": "2022-03-30T12:50:48.713Z",
            "modified": "2022-03-30T12:50:48.713Z"
        },
        {
            "name": "Discover ML Model Family",
            "description": "Adversaries may discover the general family of model.\nGeneral information about the model may be revealed in documentation, or the adversary may used carefully constructed examples and analyze the model's responses to categorize it.\n\nKnowledge of the model family can help the adversary identify means of attacking the model and help tailor the attack.\n",
            "kill_chain_phases": [
                {
                    "kill_chain_name": "mitre-atlas",
                    "phase_name": "discovery"
                }
            ],
            "external_references": [
                {
                    "source_name": "mitre-atlas",
                    "url": "https://localhost:4200/techniques/AML.T0014",
                    "external_id": "AML.T0014"
                }
            ],
            "x_mitre_platforms": [
                "ATLAS"
            ],
            "type": "attack-pattern",
            "id": "attack-pattern--6f04d364-e449-44ba-b2fd-56e716f9b0a4",
            "created": "2022-03-30T12:50:48.713Z",
            "modified": "2022-03-30T12:50:48.713Z"
        },
        {
            "name": "Poison Training Data",
            "description": "Adversaries may attempt to poison datasets used by a ML model by modifying the underlying data or its labels.\nThis allows the adversary to embed vulnerabilities in ML models trained on the data that may not be easily detectable.\nData poisoning attacks may or may not require modifying the labels.\nThe embedded vulnerability is activated at a later time by data samples with an [Insert Backdoor Trigger](/techniques/AML.T0043.004)\n\nPoisoned data can be introduced via [ML Supply Chain Compromise](/techniques/AML.T0010) or the data may be poisoned after the adversary gains [Initial Access](/tactics/AML.TA0004) to the system.\n",
            "kill_chain_phases": [
                {
                    "kill_chain_name": "mitre-atlas",
                    "phase_name": "resource-development"
                },
                {
                    "kill_chain_name": "mitre-atlas",
                    "phase_name": "persistence"
                }
            ],
            "external_references": [
                {
                    "source_name": "mitre-atlas",
                    "url": "https://localhost:4200/techniques/AML.T0020",
                    "external_id": "AML.T0020"
                }
            ],
            "x_mitre_platforms": [
                "ATLAS"
            ],
            "type": "attack-pattern",
            "id": "attack-pattern--70185f77-9e0f-4712-acc2-07a12de11078",
            "created": "2022-03-30T12:50:48.713Z",
            "modified": "2022-03-30T12:50:48.713Z"
        },
        {
            "name": "Establish Accounts",
            "description": "Adversaries may create accounts with various services for use in targeting, to gain access to resources needed in [ML Attack Staging](/tactics/AML.TA0001), or for victim impersonation.\n",
            "kill_chain_phases": [
                {
                    "kill_chain_name": "mitre-atlas",
                    "phase_name": "resource-development"
                }
            ],
            "external_references": [
                {
                    "source_name": "mitre-atlas",
                    "url": "https://localhost:4200/techniques/AML.T0021",
                    "external_id": "AML.T0021"
                }
            ],
            "x_mitre_platforms": [
                "ATLAS"
            ],
            "type": "attack-pattern",
            "id": "attack-pattern--aafec52a-dab5-4d87-b8bb-dfed0e948465",
            "created": "2022-03-30T12:50:48.713Z",
            "modified": "2022-03-30T12:50:48.713Z"
        },
        {
            "name": "Create Proxy ML Model",
            "description": "Adversaries may obtain models to serve as proxies for the target model in use at the victim organization.\nProxy models are used to simulate complete access to the target model in a fully offline manner.\n\nAdversaries may train models from representative datasets, attempt to replicate models from victim inference APIs, or use available pre-trained models.\n",
            "kill_chain_phases": [
                {
                    "kill_chain_name": "mitre-atlas",
                    "phase_name": "ml-attack-staging"
                }
            ],
            "external_references": [
                {
                    "source_name": "mitre-atlas",
                    "url": "https://localhost:4200/techniques/AML.T0005",
                    "external_id": "AML.T0005"
                }
            ],
            "x_mitre_platforms": [
                "ATLAS"
            ],
            "type": "attack-pattern",
            "id": "attack-pattern--24ab3278-6a4a-4515-8bc5-47945e915a6f",
            "created": "2022-03-30T12:50:48.713Z",
            "modified": "2022-03-30T12:50:48.713Z"
        },
        {
            "name": "Train Proxy via Gathered ML Artifacts",
            "description": "Proxy models may be trained from ML artifacts (such as data, model architectures, and pre-trained models) that are representative of the target model gathered by the adversary.\nThis can be used to develop attacks that require higher levels of access than the adversary has available or as a means to validate pre-existing attacks without interacting with the target model.\n",
            "kill_chain_phases": [
                {
                    "kill_chain_name": "mitre-atlas",
                    "phase_name": "ml-attack-staging"
                }
            ],
            "external_references": [
                {
                    "source_name": "mitre-atlas",
                    "url": "https://localhost:4200/techniques/AML.T0005.000",
                    "external_id": "AML.T0005.000"
                }
            ],
            "x_mitre_platforms": [
                "ATLAS"
            ],
            "x_mitre_is_subtechnique": true,
            "type": "attack-pattern",
            "id": "attack-pattern--95acdfcb-958d-433a-b2cb-88c4c94b2019",
            "created": "2022-03-30T12:50:48.713Z",
            "modified": "2022-03-30T12:50:48.713Z"
        },
        {
            "name": "Train Proxy via Replication",
            "description": "Adversaries may replicate a private model.\nBy repeatedly querying the victim's [ML Model Inference API Access](/techniques/AML.T0040), the adversary can collect the target model's inferences into a dataset.\nThe inferences are used as labels for training a separate model offline that will mimic the behavior and performance of the target model.\n\nA replicated model that closely mimic's the target model is a valuable resource in staging the attack.\nThe adversary can use the replicated model to [Craft Adversarial Data](/techniques/AML.T0043) for various purposes (e.g. [Evade ML Model](/techniques/AML.T0015), [Spamming ML System with Chaff Data](/techniques/AML.T0046)).\n",
            "kill_chain_phases": [
                {
                    "kill_chain_name": "mitre-atlas",
                    "phase_name": "ml-attack-staging"
                }
            ],
            "external_references": [
                {
                    "source_name": "mitre-atlas",
                    "url": "https://localhost:4200/techniques/AML.T0005.001",
                    "external_id": "AML.T0005.001"
                }
            ],
            "x_mitre_platforms": [
                "ATLAS"
            ],
            "x_mitre_is_subtechnique": true,
            "type": "attack-pattern",
            "id": "attack-pattern--cec3c76f-bd62-41ab-a20c-da83e97c69c9",
            "created": "2022-03-30T12:50:48.714Z",
            "modified": "2022-03-30T12:50:48.714Z"
        },
        {
            "name": "Use Pre-Trained Model",
            "description": "Adversaries may use an off-the-shelf pre-trained model as a proxy for the victim model to aid in staging the attack.\n",
            "kill_chain_phases": [
                {
                    "kill_chain_name": "mitre-atlas",
                    "phase_name": "ml-attack-staging"
                }
            ],
            "external_references": [
                {
                    "source_name": "mitre-atlas",
                    "url": "https://localhost:4200/techniques/AML.T0005.002",
                    "external_id": "AML.T0005.002"
                }
            ],
            "x_mitre_platforms": [
                "ATLAS"
            ],
            "x_mitre_is_subtechnique": true,
            "type": "attack-pattern",
            "id": "attack-pattern--b843f76d-0e02-4398-8be0-0dc0c2d04855",
            "created": "2022-03-30T12:50:48.714Z",
            "modified": "2022-03-30T12:50:48.714Z"
        },
        {
            "name": "Discover ML Artifacts",
            "description": "Adversaries may search private sources to identify machine learning artifacts that exist on the system and gather information about them.\nThese artifacts can include the software stack used to train and deploy models, training and testing data management systems, container registries, software repositories, and model zoos.\n\nThis information can be used to identify targets for further collection, exfiltration, or disruption, and to tailor and improve attacks.\n",
            "kill_chain_phases": [
                {
                    "kill_chain_name": "mitre-atlas",
                    "phase_name": "discovery"
                }
            ],
            "external_references": [
                {
                    "source_name": "mitre-atlas",
                    "url": "https://localhost:4200/techniques/AML.T0007",
                    "external_id": "AML.T0007"
                }
            ],
            "x_mitre_platforms": [
                "ATLAS"
            ],
            "type": "attack-pattern",
            "id": "attack-pattern--7d7237f2-9495-49fe-83bd-beb609e0242b",
            "created": "2022-03-30T12:50:48.714Z",
            "modified": "2022-03-30T12:50:48.714Z"
        },
        {
            "name": "User Execution",
            "description": "An adversary may rely upon specific actions by a user in order to gain execution.\nUsers may inadvertently execute unsafe code introduced via [ML Supply Chain Compromise](/techniques/AML.T0010).\nUsers may be subjected to social engineering to get them to execute malicious code by, for example, opening a malicious document file or link.\n",
            "kill_chain_phases": [
                {
                    "kill_chain_name": "mitre-atlas",
                    "phase_name": "execution"
                }
            ],
            "external_references": [
                {
                    "source_name": "mitre-atlas",
                    "url": "https://localhost:4200/techniques/AML.T0011",
                    "external_id": "AML.T0011"
                }
            ],
            "x_mitre_platforms": [
                "ATLAS"
            ],
            "type": "attack-pattern",
            "id": "attack-pattern--76e1c20c-29fd-4c90-8f8a-fbb9635aa5fd",
            "created": "2022-03-30T12:50:48.714Z",
            "modified": "2022-03-30T12:50:48.714Z"
        },
        {
            "name": "Unsafe ML Artifacts",
            "description": "Adversaries may develop unsafe ML artifacts that when executed have a deleterious effect.\nThe adversary can use this technique to establish persistent access to systems.\nThese models may be introduced via a [ML Supply Chain Compromise](/techniques/AML.T0010).\n\nSerialization of models is a popular technique for model storage, transfer, and loading.\nHowever, this format without proper checking presents an opportunity for code execution.\n",
            "kill_chain_phases": [
                {
                    "kill_chain_name": "mitre-atlas",
                    "phase_name": "execution"
                }
            ],
            "external_references": [
                {
                    "source_name": "mitre-atlas",
                    "url": "https://localhost:4200/techniques/AML.T0011.000",
                    "external_id": "AML.T0011.000"
                }
            ],
            "x_mitre_platforms": [
                "ATLAS"
            ],
            "x_mitre_is_subtechnique": true,
            "type": "attack-pattern",
            "id": "attack-pattern--8c69b300-825f-45bc-86a3-9f664d7bc619",
            "created": "2022-03-30T12:50:48.715Z",
            "modified": "2022-03-30T12:50:48.715Z"
        },
        {
            "name": "Valid Accounts",
            "description": "Adversaries may obtain and abuse credentials of existing accounts as a means of gaining Initial Access.\nCredentials may take the form of usernames and passwords of individual user accounts or API keys that provide access to various ML resources and services.\n\nCompromised credentials may provide access to additional ML artifacts and allow the adversary to perform [Discover ML Artifacts](/techniques/AML.T0007).\nCompromised credentials may also grant and adversary increased privileges such as write access to ML artifacts used during development or production.\n",
            "kill_chain_phases": [
                {
                    "kill_chain_name": "mitre-atlas",
                    "phase_name": "initial-access"
                }
            ],
            "external_references": [
                {
                    "source_name": "mitre-atlas",
                    "url": "https://localhost:4200/techniques/AML.T0012",
                    "external_id": "AML.T0012"
                }
            ],
            "x_mitre_platforms": [
                "ATLAS"
            ],
            "type": "attack-pattern",
            "id": "attack-pattern--ecf78bbb-fe2d-4e5c-8398-842ffeb4dfb4",
            "created": "2022-03-30T12:50:48.715Z",
            "modified": "2022-03-30T12:50:48.715Z"
        },
        {
            "name": "Evade ML Model",
            "description": "Adversaries can [Craft Adversarial Data](/techniques/AML.T0043) that prevent a machine learning model from correctly identifying the contents of the data.\nThis technique can be used to evade a downstream task where machine learning is utilized.\nThe adversary may evade machine learning based virus/malware detection, or network scanning towards the goal of a traditional cyber attack.\n",
            "kill_chain_phases": [
                {
                    "kill_chain_name": "mitre-atlas",
                    "phase_name": "defense-evasion"
                },
                {
                    "kill_chain_name": "mitre-atlas",
                    "phase_name": "impact"
                }
            ],
            "external_references": [
                {
                    "source_name": "mitre-atlas",
                    "url": "https://localhost:4200/techniques/AML.T0015",
                    "external_id": "AML.T0015"
                }
            ],
            "x_mitre_platforms": [
                "ATLAS"
            ],
            "type": "attack-pattern",
            "id": "attack-pattern--e84e7ae4-4b7a-474a-b8d9-41488d991083",
            "created": "2022-03-30T12:50:48.715Z",
            "modified": "2022-03-30T12:50:48.715Z"
        },
        {
            "name": "Backdoor ML Model",
            "description": "Adversaries may introduce a backdoor into a ML model.\nA backdoored model operates performs as expected under typical conditions, but will produce the adversary's desired output when a trigger is introduced to the input data.\nA backdoored model provides the adversary with a persistent artifact on the victim system.\nThe embedded vulnerability is typically activated at a later time by data samples with an [Insert Backdoor Trigger](/techniques/AML.T0043.004)\n",
            "kill_chain_phases": [
                {
                    "kill_chain_name": "mitre-atlas",
                    "phase_name": "persistence"
                },
                {
                    "kill_chain_name": "mitre-atlas",
                    "phase_name": "ml-attack-staging"
                }
            ],
            "external_references": [
                {
                    "source_name": "mitre-atlas",
                    "url": "https://localhost:4200/techniques/AML.T0018",
                    "external_id": "AML.T0018"
                }
            ],
            "x_mitre_platforms": [
                "ATLAS"
            ],
            "type": "attack-pattern",
            "id": "attack-pattern--c825e7e6-6200-40ec-b3f4-2a0529edf267",
            "created": "2022-03-30T12:50:48.716Z",
            "modified": "2022-03-30T12:50:48.716Z"
        },
        {
            "name": "Poison ML Model",
            "description": "Adversaries may introduce a backdoor by training the model poisoned data, or by interfering with its training process.\nThe model learns to associate a adversary defined trigger with the adversary's desired output.\n",
            "kill_chain_phases": [
                {
                    "kill_chain_name": "mitre-atlas",
                    "phase_name": "persistence"
                },
                {
                    "kill_chain_name": "mitre-atlas",
                    "phase_name": "ml-attack-staging"
                }
            ],
            "external_references": [
                {
                    "source_name": "mitre-atlas",
                    "url": "https://localhost:4200/techniques/AML.T0018.000",
                    "external_id": "AML.T0018.000"
                }
            ],
            "x_mitre_platforms": [
                "ATLAS"
            ],
            "x_mitre_is_subtechnique": true,
            "type": "attack-pattern",
            "id": "attack-pattern--19f2670b-7f50-4996-be78-218fd3193ab4",
            "created": "2022-03-30T12:50:48.716Z",
            "modified": "2022-03-30T12:50:48.716Z"
        },
        {
            "name": "Inject Payload",
            "description": "Adversaries may introduce a backdoor into a model by injecting a payload into the model file.\nThe payload detects the presence of the trigger and bypasses the model, instead producing the adversary's desired output.\n",
            "kill_chain_phases": [
                {
                    "kill_chain_name": "mitre-atlas",
                    "phase_name": "persistence"
                },
                {
                    "kill_chain_name": "mitre-atlas",
                    "phase_name": "ml-attack-staging"
                }
            ],
            "external_references": [
                {
                    "source_name": "mitre-atlas",
                    "url": "https://localhost:4200/techniques/AML.T0018.001",
                    "external_id": "AML.T0018.001"
                }
            ],
            "x_mitre_platforms": [
                "ATLAS"
            ],
            "x_mitre_is_subtechnique": true,
            "type": "attack-pattern",
            "id": "attack-pattern--91c315ee-15cd-476e-bf2c-f87305c1fdda",
            "created": "2022-03-30T12:50:48.716Z",
            "modified": "2022-03-30T12:50:48.716Z"
        },
        {
            "name": "Exfiltration via ML Inference API",
            "description": "Adversaries may exfiltrate private information via [ML Model Inference API Access](/techniques/AML.T0040).\nML Models have been shown leak private information about their training data (e.g.  [Infer Training Data Membership](/techniques/AML.T0024.000), [Invert ML Model](/techniques/AML.T0024.001)).\nThe model itself may also be extracted ([Extract ML Model](/techniques/AML.T0024.002)) for the purposes of [ML Intellectual Property Theft](/techniques/AML.T0045).\n\nExfiltration of information relating to private training data raises privacy concerns.\nPrivate training data may include personally identifiable information, or other protected data.\n",
            "kill_chain_phases": [
                {
                    "kill_chain_name": "mitre-atlas",
                    "phase_name": "exfiltration"
                }
            ],
            "external_references": [
                {
                    "source_name": "mitre-atlas",
                    "url": "https://localhost:4200/techniques/AML.T0024",
                    "external_id": "AML.T0024"
                }
            ],
            "x_mitre_platforms": [
                "ATLAS"
            ],
            "type": "attack-pattern",
            "id": "attack-pattern--1d62d80e-7386-4fbc-a709-162c8987357a",
            "created": "2022-03-30T12:50:48.717Z",
            "modified": "2022-03-30T12:50:48.717Z"
        },
        {
            "name": "Infer Training Data Membership",
            "description": "Adversaries may infer the membership of a data sample in its training set, which raises privacy concerns.\nSome strategies make use of a shadow model that could be obtained via [Train Proxy via Replication](/techniques/AML.T0005.001), others use statistics of model prediction scores.\n\nThis can cause the victim model to leak private information, such as PII of those in the training set or other forms of protected IP.\n",
            "kill_chain_phases": [
                {
                    "kill_chain_name": "mitre-atlas",
                    "phase_name": "exfiltration"
                }
            ],
            "external_references": [
                {
                    "source_name": "mitre-atlas",
                    "url": "https://localhost:4200/techniques/AML.T0024.000",
                    "external_id": "AML.T0024.000"
                }
            ],
            "x_mitre_platforms": [
                "ATLAS"
            ],
            "x_mitre_is_subtechnique": true,
            "type": "attack-pattern",
            "id": "attack-pattern--f56f2780-907a-443f-97fe-3a87f0d1b8d5",
            "created": "2022-03-30T12:50:48.717Z",
            "modified": "2022-03-30T12:50:48.717Z"
        },
        {
            "name": "Invert ML Model",
            "description": "Machine learning models' training data could be reconstructed by exploiting the confidence scores that are available via an inference API.\nBy querying the inference API strategically, adversaries can back out potentially private information embedded within the training data.\nThis could lead to privacy violations if the attacker can reconstruct the data of sensitive features used in the algorithm.\n",
            "kill_chain_phases": [
                {
                    "kill_chain_name": "mitre-atlas",
                    "phase_name": "exfiltration"
                }
            ],
            "external_references": [
                {
                    "source_name": "mitre-atlas",
                    "url": "https://localhost:4200/techniques/AML.T0024.001",
                    "external_id": "AML.T0024.001"
                }
            ],
            "x_mitre_platforms": [
                "ATLAS"
            ],
            "x_mitre_is_subtechnique": true,
            "type": "attack-pattern",
            "id": "attack-pattern--63b72b56-5062-4982-9496-6ac95e38235e",
            "created": "2022-03-30T12:50:48.717Z",
            "modified": "2022-03-30T12:50:48.717Z"
        },
        {
            "name": "Extract ML Model",
            "description": "Adversaries may extract a functional copy of a private model.\nBy repeatedly querying the victim's [ML Model Inference API Access](/techniques/AML.T0040), the adversary can collect the target model's inferences into a dataset.\nThe inferences are used as labels for training a separate model offline that will mimic the behavior and performance of the target model.\n\nAdversaries may extract the model to avoid paying per query in a machine learning as a service setting.\nModel extraction is used for [ML Intellectual Property Theft](/techniques/AML.T0045).\n",
            "kill_chain_phases": [
                {
                    "kill_chain_name": "mitre-atlas",
                    "phase_name": "exfiltration"
                }
            ],
            "external_references": [
                {
                    "source_name": "mitre-atlas",
                    "url": "https://localhost:4200/techniques/AML.T0024.002",
                    "external_id": "AML.T0024.002"
                }
            ],
            "x_mitre_platforms": [
                "ATLAS"
            ],
            "x_mitre_is_subtechnique": true,
            "type": "attack-pattern",
            "id": "attack-pattern--111e75ea-704e-48ec-9f49-dc7ccb98dc90",
            "created": "2022-03-30T12:50:48.718Z",
            "modified": "2022-03-30T12:50:48.718Z"
        },
        {
            "name": "Exfiltration via Cyber Means",
            "description": "Adversaries may exfiltrate ML artifacts or other information relevant to their goals via traditional cyber means.\n\nSee the ATT&CK [Exfiltration](https://attack.mitre.org/tactics/TA0010/) tactic for more information.\n",
            "kill_chain_phases": [
                {
                    "kill_chain_name": "mitre-atlas",
                    "phase_name": "exfiltration"
                }
            ],
            "external_references": [
                {
                    "source_name": "mitre-atlas",
                    "url": "https://localhost:4200/techniques/AML.T0025",
                    "external_id": "AML.T0025"
                }
            ],
            "x_mitre_platforms": [
                "ATLAS"
            ],
            "type": "attack-pattern",
            "id": "attack-pattern--23e321d8-38c4-4b74-800c-fecc2d26c12e",
            "created": "2022-03-30T12:50:48.718Z",
            "modified": "2022-03-30T12:50:48.718Z"
        },
        {
            "name": "Denial of ML Service",
            "description": "Adversaries may target machine learning systems with a flood of requests for the purpose of degrading or shutting down the service.\nSince many machine learning systems require significant amounts of specialized compute, they are often expensive bottlenecks that can become overloaded.\nAdversaries can intentionally craft inputs that require heavy amounts of useless compute from the machine learning system.\n",
            "kill_chain_phases": [
                {
                    "kill_chain_name": "mitre-atlas",
                    "phase_name": "impact"
                }
            ],
            "external_references": [
                {
                    "source_name": "mitre-atlas",
                    "url": "https://localhost:4200/techniques/AML.T0029",
                    "external_id": "AML.T0029"
                }
            ],
            "x_mitre_platforms": [
                "ATLAS"
            ],
            "type": "attack-pattern",
            "id": "attack-pattern--5d8bd6fa-3542-4b44-82d1-ce7d201a5b19",
            "created": "2022-03-30T12:50:48.718Z",
            "modified": "2022-03-30T12:50:48.718Z"
        },
        {
            "name": "Spamming ML System with Chaff Data",
            "description": "Adversaries may spam the machine learning system with chaff data that causes increase in the number of detections.\nThis can cause analysts at the victim organization to waste time reviewing and correcting incorrect inferences.\n",
            "kill_chain_phases": [
                {
                    "kill_chain_name": "mitre-atlas",
                    "phase_name": "impact"
                }
            ],
            "external_references": [
                {
                    "source_name": "mitre-atlas",
                    "url": "https://localhost:4200/techniques/AML.T0046",
                    "external_id": "AML.T0046"
                }
            ],
            "x_mitre_platforms": [
                "ATLAS"
            ],
            "type": "attack-pattern",
            "id": "attack-pattern--dbd94823-c963-4341-86f2-0043d0df33ac",
            "created": "2022-03-30T12:50:48.718Z",
            "modified": "2022-03-30T12:50:48.718Z"
        },
        {
            "name": "Erode ML Model Integrity",
            "description": "Adversaries may degrade the target model's performance with adversarial data inputs to erode confidence in the system over time.\nThis can lead to the victim organization wasting time and money both attempting to fix the system and performing the tasks it was meant to automate by hand.\n",
            "kill_chain_phases": [
                {
                    "kill_chain_name": "mitre-atlas",
                    "phase_name": "impact"
                }
            ],
            "external_references": [
                {
                    "source_name": "mitre-atlas",
                    "url": "https://localhost:4200/techniques/AML.T0031",
                    "external_id": "AML.T0031"
                }
            ],
            "x_mitre_platforms": [
                "ATLAS"
            ],
            "type": "attack-pattern",
            "id": "attack-pattern--91d09329-6aa0-45e0-ac16-9caeb2b78f5a",
            "created": "2022-03-30T12:50:48.718Z",
            "modified": "2022-03-30T12:50:48.718Z"
        },
        {
            "name": "Cost Harvesting",
            "description": "Adversaries may target different machine learning services to send useless queries or computationally expensive inputs to increase the cost of running services at the victim organization.\nSponge examples are a particular type of adversarial data designed to maximize energy consumption and thus operating cost.\n",
            "kill_chain_phases": [
                {
                    "kill_chain_name": "mitre-atlas",
                    "phase_name": "impact"
                }
            ],
            "external_references": [
                {
                    "source_name": "mitre-atlas",
                    "url": "https://localhost:4200/techniques/AML.T0034",
                    "external_id": "AML.T0034"
                }
            ],
            "x_mitre_platforms": [
                "ATLAS"
            ],
            "type": "attack-pattern",
            "id": "attack-pattern--3d1b4f28-51bb-4546-af62-9401f0269286",
            "created": "2022-03-30T12:50:48.719Z",
            "modified": "2022-03-30T12:50:48.719Z"
        },
        {
            "name": "ML Artifact Collection",
            "description": "Adversaries may collect ML artifacts for [Exfiltration](/tactics/AML.TA0010) or for use in [ML Attack Staging](/tactics/AML.TA0001).\nML artifacts include models and datasets as well as other telemetry data produced when interacting with a model.\n",
            "kill_chain_phases": [
                {
                    "kill_chain_name": "mitre-atlas",
                    "phase_name": "collection"
                }
            ],
            "external_references": [
                {
                    "source_name": "mitre-atlas",
                    "url": "https://localhost:4200/techniques/AML.T0035",
                    "external_id": "AML.T0035"
                }
            ],
            "x_mitre_platforms": [
                "ATLAS"
            ],
            "type": "attack-pattern",
            "id": "attack-pattern--4f31dacc-e7c3-4492-b45b-6ea70c5a6d67",
            "created": "2022-03-30T12:50:48.719Z",
            "modified": "2022-03-30T12:50:48.719Z"
        },
        {
            "name": "Data from Information Repositories",
            "description": "Adversaries may leverage information repositories to mine valuable information.\nInformation repositories are tools that allow for storage of information, typically to facilitate collaboration or information sharing between users, and can store a wide variety of data that may aid adversaries in further objectives, or direct access to the target information.\n\nInformation stored in a repository may vary based on the specific instance or environment.\nSpecific common information repositories include Sharepoint, Confluence, and enterprise databases such as SQL Server.\n",
            "kill_chain_phases": [
                {
                    "kill_chain_name": "mitre-atlas",
                    "phase_name": "collection"
                }
            ],
            "external_references": [
                {
                    "source_name": "mitre-atlas",
                    "url": "https://localhost:4200/techniques/AML.T0036",
                    "external_id": "AML.T0036"
                }
            ],
            "x_mitre_platforms": [
                "ATLAS"
            ],
            "type": "attack-pattern",
            "id": "attack-pattern--59708f87-df05-4426-89f4-63323f2e755b",
            "created": "2022-03-30T12:50:48.719Z",
            "modified": "2022-03-30T12:50:48.719Z"
        },
        {
            "name": "Verify Attack",
            "description": "Adversaries can verify the efficacy of their attack via an inference API or access to an offline copy of the target model.\nThis gives the adversary confidence that their approach works and allows them to carry out the attack at a later time of their choosing.\nThe adversary may verify the attack once but use it against many edge devices running copies of the target model.\nThe adversary may verify their attack digitally, then deploy it in the [Physical Environment Access](/techniques/AML.T0041) at a later time.\nVerifying the attack may be hard to detect since the adversary can use a minimal number of queries or an offline copy of the model.\n",
            "kill_chain_phases": [
                {
                    "kill_chain_name": "mitre-atlas",
                    "phase_name": "ml-attack-staging"
                }
            ],
            "external_references": [
                {
                    "source_name": "mitre-atlas",
                    "url": "https://localhost:4200/techniques/AML.T0042",
                    "external_id": "AML.T0042"
                }
            ],
            "x_mitre_platforms": [
                "ATLAS"
            ],
            "type": "attack-pattern",
            "id": "attack-pattern--e0ae3359-9b0b-48e4-b77c-b1f2ebeb5ff3",
            "created": "2022-03-30T12:50:48.719Z",
            "modified": "2022-03-30T12:50:48.719Z"
        },
        {
            "name": "Craft Adversarial Data",
            "description": "Adversarial data are inputs to a machine learning model that have been modified such that they cause the adversary's desired effect in the target model.\nEffects can range from misclassification, to missed detections, to maximising energy consumption.\nTypically, the modification is constrained in magnitude or location so that a human still perceives the data as if it were unmodified, but human perceptibility may not always be a concern depending on the adversary's intended effect.\nFor example, an adversarial input for an image classification task is an image the machine learning model would misclassify, but a human would still recognize as containing the correct class.\n\nDepending on the adversary's knowledge of and access to the target model, the adversary may use different classes of algorithms to develop the adversarial example such as [White-Box Optimization](/techniques/AML.T0043.000), [Black-Box Optimization](/techniques/AML.T0043.001), [Black-Box Transfer](/techniques/AML.T0043.002), or [Manual Modification](/techniques/AML.T0043.003).\n\nThe adversary may [Verify Attack](/techniques/AML.T0042) their approach works if they have white-box or inference API access to the model.\nThis allows the adversary to gain confidence their attack is effective \"live\" environment where their attack may be noticed.\nThey can then use the attack at a later time to accomplish their goals.\nAn adversary may optimize adversarial examples for [Evade ML Model](/techniques/AML.T0015), or to [Erode ML Model Integrity](/techniques/AML.T0031).\n",
            "kill_chain_phases": [
                {
                    "kill_chain_name": "mitre-atlas",
                    "phase_name": "ml-attack-staging"
                }
            ],
            "external_references": [
                {
                    "source_name": "mitre-atlas",
                    "url": "https://localhost:4200/techniques/AML.T0043",
                    "external_id": "AML.T0043"
                }
            ],
            "x_mitre_platforms": [
                "ATLAS"
            ],
            "type": "attack-pattern",
            "id": "attack-pattern--5194796b-5f23-4524-8179-0e4cdbd1c34a",
            "created": "2022-03-30T12:50:48.719Z",
            "modified": "2022-03-30T12:50:48.719Z"
        },
        {
            "name": "White-Box Optimization",
            "description": "In White-Box Optimization, the adversary has full access to the target model and optimizes the adversarial example directly.\nAdversarial examples trained in this manor are most effective against the target model.\n",
            "kill_chain_phases": [
                {
                    "kill_chain_name": "mitre-atlas",
                    "phase_name": "ml-attack-staging"
                }
            ],
            "external_references": [
                {
                    "source_name": "mitre-atlas",
                    "url": "https://localhost:4200/techniques/AML.T0043.000",
                    "external_id": "AML.T0043.000"
                }
            ],
            "x_mitre_platforms": [
                "ATLAS"
            ],
            "x_mitre_is_subtechnique": true,
            "type": "attack-pattern",
            "id": "attack-pattern--5569f6eb-bb7c-44ac-b8cf-c4d21a2c2ce6",
            "created": "2022-03-30T12:50:48.719Z",
            "modified": "2022-03-30T12:50:48.719Z"
        },
        {
            "name": "Black-Box Optimization",
            "description": "In Black-Box attacks, the adversary has black-box (i.e. [ML Model Inference API Access](/techniques/AML.T0040) via API access) access to the target model.\nWith black-box attacks, the adversary may be using an API that the victim is monitoring.\nThese attacks are generally less effective and require more inferences than [White-Box Optimization](/techniques/AML.T0043.000) attacks, but they require much less access.\n",
            "kill_chain_phases": [
                {
                    "kill_chain_name": "mitre-atlas",
                    "phase_name": "ml-attack-staging"
                }
            ],
            "external_references": [
                {
                    "source_name": "mitre-atlas",
                    "url": "https://localhost:4200/techniques/AML.T0043.001",
                    "external_id": "AML.T0043.001"
                }
            ],
            "x_mitre_platforms": [
                "ATLAS"
            ],
            "x_mitre_is_subtechnique": true,
            "type": "attack-pattern",
            "id": "attack-pattern--6dd7d96a-2fa8-41a3-92aa-5fc85f813c56",
            "created": "2022-03-30T12:50:48.720Z",
            "modified": "2022-03-30T12:50:48.720Z"
        },
        {
            "name": "Black-Box Transfer",
            "description": "In Black-Box Transfer attacks, the adversary uses one or more proxy models (trained via [Create Proxy ML Model](/techniques/AML.T0005) or [Train Proxy via Replication](/techniques/AML.T0005.001)) models they have full access to and are representative of the target model.\nThe adversary uses [White-Box Optimization](/techniques/AML.T0043.000) on the proxy models to generate adversarial examples.\nIf the set of proxy models are close enough to the target model, the adversarial example should generalize from one to another.\nThis means that an attack that works for the proxy models will likely then work for the target model.\nIf the adversary has [ML Model Inference API Access](/techniques/AML.T0040), they may use this [Verify Attack](/techniques/AML.T0042) that the attack is working and incorporate that information into their training process.\n",
            "kill_chain_phases": [
                {
                    "kill_chain_name": "mitre-atlas",
                    "phase_name": "ml-attack-staging"
                }
            ],
            "external_references": [
                {
                    "source_name": "mitre-atlas",
                    "url": "https://localhost:4200/techniques/AML.T0043.002",
                    "external_id": "AML.T0043.002"
                }
            ],
            "x_mitre_platforms": [
                "ATLAS"
            ],
            "x_mitre_is_subtechnique": true,
            "type": "attack-pattern",
            "id": "attack-pattern--f5c0a229-7474-4d16-a76d-309719efca22",
            "created": "2022-03-30T12:50:48.720Z",
            "modified": "2022-03-30T12:50:48.720Z"
        },
        {
            "name": "Manual Modification",
            "description": "Adversaries may manually modify the input data to craft adversarial data.\nThey may use their knowledge of the target model to modify parts of the data they suspect helps the model in performing its task.\nThe adversary may use trial and error until they are able to verify they have a working adversarial input.\n",
            "kill_chain_phases": [
                {
                    "kill_chain_name": "mitre-atlas",
                    "phase_name": "ml-attack-staging"
                }
            ],
            "external_references": [
                {
                    "source_name": "mitre-atlas",
                    "url": "https://localhost:4200/techniques/AML.T0043.003",
                    "external_id": "AML.T0043.003"
                }
            ],
            "x_mitre_platforms": [
                "ATLAS"
            ],
            "x_mitre_is_subtechnique": true,
            "type": "attack-pattern",
            "id": "attack-pattern--9476649b-5fe1-4de9-aa7c-06ca53ff8323",
            "created": "2022-03-30T12:50:48.720Z",
            "modified": "2022-03-30T12:50:48.720Z"
        },
        {
            "name": "Insert Backdoor Trigger",
            "description": "The adversary may add a perceptual trigger into inference data.\nThe trigger may be imperceptible or non-obvious to humans.\nThis technique is used in conjunction with [Poison ML Model](/techniques/AML.T0018.000) and allows the adversary to produce their desired effect in the target model.\n",
            "kill_chain_phases": [
                {
                    "kill_chain_name": "mitre-atlas",
                    "phase_name": "ml-attack-staging"
                }
            ],
            "external_references": [
                {
                    "source_name": "mitre-atlas",
                    "url": "https://localhost:4200/techniques/AML.T0043.004",
                    "external_id": "AML.T0043.004"
                }
            ],
            "x_mitre_platforms": [
                "ATLAS"
            ],
            "x_mitre_is_subtechnique": true,
            "type": "attack-pattern",
            "id": "attack-pattern--aad9054d-a3c4-424e-9610-56667ff6cad4",
            "created": "2022-03-30T12:50:48.720Z",
            "modified": "2022-03-30T12:50:48.720Z"
        },
        {
            "name": "ML Intellectual Property Theft",
            "description": "Adversaries may exfiltrate ML artifacts to steal intellectual property and cause economic harm to the victim organization.\n\nProprietary training data is costly to collect and annotate and may be a target for [Exfiltration](/tactics/AML.TA0010) and theft.\n\nMLaaS providers charge for use of their API.\nAn adversary who has stolen a model via [Exfiltration](/tactics/AML.TA0010) or via [Extract ML Model](/techniques/AML.T0024.002) now has unlimited use of that service without paying the owner of the intellectual property.\n",
            "kill_chain_phases": [
                {
                    "kill_chain_name": "mitre-atlas",
                    "phase_name": "impact"
                }
            ],
            "external_references": [
                {
                    "source_name": "mitre-atlas",
                    "url": "https://localhost:4200/techniques/AML.T0045",
                    "external_id": "AML.T0045"
                }
            ],
            "x_mitre_platforms": [
                "ATLAS"
            ],
            "type": "attack-pattern",
            "id": "attack-pattern--cf637acf-6c16-40a0-b788-22939d810649",
            "created": "2022-03-30T12:50:48.720Z",
            "modified": "2022-03-30T12:50:48.720Z"
        },
        {
            "type": "relationship",
            "id": "relationship--20d3847c-5c32-4ded-96b2-cd4515ea28d7",
            "created": "2022-03-30T12:50:48.707Z",
            "modified": "2022-03-30T12:50:48.707Z",
            "source_ref": "attack-pattern--38e5993d-7751-4dd3-a88e-d5bcb6ca105d",
            "relationship_type": "subtechnique-of",
            "target_ref": "attack-pattern--a80307ac-eb6b-496f-8fe1-b36e6b83caf2"
        },
        {
            "type": "relationship",
            "id": "relationship--620d282d-709b-4f36-83a1-5a1802f24a03",
            "created": "2022-03-30T12:50:48.708Z",
            "modified": "2022-03-30T12:50:48.708Z",
            "source_ref": "attack-pattern--4bf5671b-c3fd-4170-a949-69c1c14276c7",
            "relationship_type": "subtechnique-of",
            "target_ref": "attack-pattern--a80307ac-eb6b-496f-8fe1-b36e6b83caf2"
        },
        {
            "type": "relationship",
            "id": "relationship--b97efc80-d405-40e3-a691-1aed853c6bce",
            "created": "2022-03-30T12:50:48.708Z",
            "modified": "2022-03-30T12:50:48.708Z",
            "source_ref": "attack-pattern--5d522173-f322-433e-bfca-694cd739c30b",
            "relationship_type": "subtechnique-of",
            "target_ref": "attack-pattern--a80307ac-eb6b-496f-8fe1-b36e6b83caf2"
        },
        {
            "type": "relationship",
            "id": "relationship--0a8f149e-5c8f-4797-96dd-aed9cbd07667",
            "created": "2022-03-30T12:50:48.709Z",
            "modified": "2022-03-30T12:50:48.709Z",
            "source_ref": "attack-pattern--3bcbf550-0e37-495d-8099-91c1f07095d7",
            "relationship_type": "subtechnique-of",
            "target_ref": "attack-pattern--56c291a6-c822-402c-bec2-10813ba60027"
        },
        {
            "type": "relationship",
            "id": "relationship--92b02122-ab09-4670-ab15-f1e19ab03a09",
            "created": "2022-03-30T12:50:48.709Z",
            "modified": "2022-03-30T12:50:48.709Z",
            "source_ref": "attack-pattern--d7253fda-7e4e-40aa-99a0-98f30d0d8312",
            "relationship_type": "subtechnique-of",
            "target_ref": "attack-pattern--56c291a6-c822-402c-bec2-10813ba60027"
        },
        {
            "type": "relationship",
            "id": "relationship--c7a0c3a9-feb6-411e-a52f-fb4ea65e433b",
            "created": "2022-03-30T12:50:48.710Z",
            "modified": "2022-03-30T12:50:48.710Z",
            "source_ref": "attack-pattern--736a7a7e-d768-4434-9ae9-b3ce83c3a7c6",
            "relationship_type": "subtechnique-of",
            "target_ref": "attack-pattern--2ca11647-62f6-4a2a-b4cf-2c7aa90a3308"
        },
        {
            "type": "relationship",
            "id": "relationship--52eae051-e8e9-4a5a-8e6c-22806b1a6daa",
            "created": "2022-03-30T12:50:48.710Z",
            "modified": "2022-03-30T12:50:48.710Z",
            "source_ref": "attack-pattern--882e2152-ea87-4bac-94bb-9a4ff39220ed",
            "relationship_type": "subtechnique-of",
            "target_ref": "attack-pattern--2ca11647-62f6-4a2a-b4cf-2c7aa90a3308"
        },
        {
            "type": "relationship",
            "id": "relationship--bf17feae-92c0-45e0-a2a0-26aecc3b7186",
            "created": "2022-03-30T12:50:48.711Z",
            "modified": "2022-03-30T12:50:48.711Z",
            "source_ref": "attack-pattern--58c08543-0c31-4eee-b786-036086f8e2db",
            "relationship_type": "subtechnique-of",
            "target_ref": "attack-pattern--f509b9d5-c2cf-4419-8e18-90e9183381a7"
        },
        {
            "type": "relationship",
            "id": "relationship--8f5dbeec-9151-442e-927f-807e0b4fbaf1",
            "created": "2022-03-30T12:50:48.711Z",
            "modified": "2022-03-30T12:50:48.711Z",
            "source_ref": "attack-pattern--3e0bb768-3661-4aa4-9704-c4db1b1d24b3",
            "relationship_type": "subtechnique-of",
            "target_ref": "attack-pattern--f509b9d5-c2cf-4419-8e18-90e9183381a7"
        },
        {
            "type": "relationship",
            "id": "relationship--c65c4b45-960e-490b-b3a3-4ec6442ce7b3",
            "created": "2022-03-30T12:50:48.711Z",
            "modified": "2022-03-30T12:50:48.711Z",
            "source_ref": "attack-pattern--db9ddb59-d7e2-4fbd-b593-68a8fd0808ac",
            "relationship_type": "subtechnique-of",
            "target_ref": "attack-pattern--b22089b2-88ff-4ac7-bd09-8585e6897e8a"
        },
        {
            "type": "relationship",
            "id": "relationship--c617128f-21c1-4adf-b15d-87445757a26e",
            "created": "2022-03-30T12:50:48.711Z",
            "modified": "2022-03-30T12:50:48.711Z",
            "source_ref": "attack-pattern--5c140d2d-193b-400a-beab-79fe8b2ee0ba",
            "relationship_type": "subtechnique-of",
            "target_ref": "attack-pattern--b22089b2-88ff-4ac7-bd09-8585e6897e8a"
        },
        {
            "type": "relationship",
            "id": "relationship--e23c2131-cdf0-4e8f-be3d-00ba768e1bee",
            "created": "2022-03-30T12:50:48.712Z",
            "modified": "2022-03-30T12:50:48.712Z",
            "source_ref": "attack-pattern--14224938-47f3-4ddb-a865-d6e3417e4bf1",
            "relationship_type": "subtechnique-of",
            "target_ref": "attack-pattern--b22089b2-88ff-4ac7-bd09-8585e6897e8a"
        },
        {
            "type": "relationship",
            "id": "relationship--c66de596-2fa1-40c2-9b7c-cbae7d6c55ab",
            "created": "2022-03-30T12:50:48.712Z",
            "modified": "2022-03-30T12:50:48.712Z",
            "source_ref": "attack-pattern--d854be46-0ae0-4744-a715-7ac593dcb618",
            "relationship_type": "subtechnique-of",
            "target_ref": "attack-pattern--b22089b2-88ff-4ac7-bd09-8585e6897e8a"
        },
        {
            "type": "relationship",
            "id": "relationship--4a07e0bc-8d02-4be4-af82-cc2bef8c25ee",
            "created": "2022-03-30T12:50:48.713Z",
            "modified": "2022-03-30T12:50:48.713Z",
            "source_ref": "attack-pattern--95acdfcb-958d-433a-b2cb-88c4c94b2019",
            "relationship_type": "subtechnique-of",
            "target_ref": "attack-pattern--24ab3278-6a4a-4515-8bc5-47945e915a6f"
        },
        {
            "type": "relationship",
            "id": "relationship--e5a081a1-357e-48f0-b00a-dcfcf64ef8b0",
            "created": "2022-03-30T12:50:48.714Z",
            "modified": "2022-03-30T12:50:48.714Z",
            "source_ref": "attack-pattern--cec3c76f-bd62-41ab-a20c-da83e97c69c9",
            "relationship_type": "subtechnique-of",
            "target_ref": "attack-pattern--24ab3278-6a4a-4515-8bc5-47945e915a6f"
        },
        {
            "type": "relationship",
            "id": "relationship--98497bdf-bd3f-4a52-91d2-cca42e18d22c",
            "created": "2022-03-30T12:50:48.714Z",
            "modified": "2022-03-30T12:50:48.714Z",
            "source_ref": "attack-pattern--b843f76d-0e02-4398-8be0-0dc0c2d04855",
            "relationship_type": "subtechnique-of",
            "target_ref": "attack-pattern--24ab3278-6a4a-4515-8bc5-47945e915a6f"
        },
        {
            "type": "relationship",
            "id": "relationship--f30eab98-cbf0-4180-85aa-26424f2af868",
            "created": "2022-03-30T12:50:48.715Z",
            "modified": "2022-03-30T12:50:48.715Z",
            "source_ref": "attack-pattern--8c69b300-825f-45bc-86a3-9f664d7bc619",
            "relationship_type": "subtechnique-of",
            "target_ref": "attack-pattern--76e1c20c-29fd-4c90-8f8a-fbb9635aa5fd"
        },
        {
            "type": "relationship",
            "id": "relationship--a0ec236d-1fce-4b41-9f65-71265a644d56",
            "created": "2022-03-30T12:50:48.716Z",
            "modified": "2022-03-30T12:50:48.716Z",
            "source_ref": "attack-pattern--19f2670b-7f50-4996-be78-218fd3193ab4",
            "relationship_type": "subtechnique-of",
            "target_ref": "attack-pattern--c825e7e6-6200-40ec-b3f4-2a0529edf267"
        },
        {
            "type": "relationship",
            "id": "relationship--cddae781-f6a2-4d20-8c2a-c08b897e4ceb",
            "created": "2022-03-30T12:50:48.716Z",
            "modified": "2022-03-30T12:50:48.716Z",
            "source_ref": "attack-pattern--91c315ee-15cd-476e-bf2c-f87305c1fdda",
            "relationship_type": "subtechnique-of",
            "target_ref": "attack-pattern--c825e7e6-6200-40ec-b3f4-2a0529edf267"
        },
        {
            "type": "relationship",
            "id": "relationship--1d92a631-b008-4717-ad95-5d2172b984cf",
            "created": "2022-03-30T12:50:48.717Z",
            "modified": "2022-03-30T12:50:48.717Z",
            "source_ref": "attack-pattern--f56f2780-907a-443f-97fe-3a87f0d1b8d5",
            "relationship_type": "subtechnique-of",
            "target_ref": "attack-pattern--1d62d80e-7386-4fbc-a709-162c8987357a"
        },
        {
            "type": "relationship",
            "id": "relationship--fc6c45fb-48c1-410e-b3aa-141aa4a7aebe",
            "created": "2022-03-30T12:50:48.717Z",
            "modified": "2022-03-30T12:50:48.717Z",
            "source_ref": "attack-pattern--63b72b56-5062-4982-9496-6ac95e38235e",
            "relationship_type": "subtechnique-of",
            "target_ref": "attack-pattern--1d62d80e-7386-4fbc-a709-162c8987357a"
        },
        {
            "type": "relationship",
            "id": "relationship--b97d87b1-2e42-4327-b8d8-270f37c4a122",
            "created": "2022-03-30T12:50:48.718Z",
            "modified": "2022-03-30T12:50:48.718Z",
            "source_ref": "attack-pattern--111e75ea-704e-48ec-9f49-dc7ccb98dc90",
            "relationship_type": "subtechnique-of",
            "target_ref": "attack-pattern--1d62d80e-7386-4fbc-a709-162c8987357a"
        },
        {
            "type": "relationship",
            "id": "relationship--c481109a-c047-4cb7-ba11-2fa931b236a2",
            "created": "2022-03-30T12:50:48.719Z",
            "modified": "2022-03-30T12:50:48.719Z",
            "source_ref": "attack-pattern--5569f6eb-bb7c-44ac-b8cf-c4d21a2c2ce6",
            "relationship_type": "subtechnique-of",
            "target_ref": "attack-pattern--5194796b-5f23-4524-8179-0e4cdbd1c34a"
        },
        {
            "type": "relationship",
            "id": "relationship--a33472a4-17f3-43ce-8662-a399168a136f",
            "created": "2022-03-30T12:50:48.720Z",
            "modified": "2022-03-30T12:50:48.720Z",
            "source_ref": "attack-pattern--6dd7d96a-2fa8-41a3-92aa-5fc85f813c56",
            "relationship_type": "subtechnique-of",
            "target_ref": "attack-pattern--5194796b-5f23-4524-8179-0e4cdbd1c34a"
        },
        {
            "type": "relationship",
            "id": "relationship--c8fe248b-1ed3-411e-a58a-fd22a5735ed5",
            "created": "2022-03-30T12:50:48.720Z",
            "modified": "2022-03-30T12:50:48.720Z",
            "source_ref": "attack-pattern--f5c0a229-7474-4d16-a76d-309719efca22",
            "relationship_type": "subtechnique-of",
            "target_ref": "attack-pattern--5194796b-5f23-4524-8179-0e4cdbd1c34a"
        },
        {
            "type": "relationship",
            "id": "relationship--58382100-a64c-4c1b-8b48-10488eb159ea",
            "created": "2022-03-30T12:50:48.720Z",
            "modified": "2022-03-30T12:50:48.720Z",
            "source_ref": "attack-pattern--9476649b-5fe1-4de9-aa7c-06ca53ff8323",
            "relationship_type": "subtechnique-of",
            "target_ref": "attack-pattern--5194796b-5f23-4524-8179-0e4cdbd1c34a"
        },
        {
            "type": "relationship",
            "id": "relationship--0f73717f-a444-4055-b771-32aaa72c685c",
            "created": "2022-03-30T12:50:48.720Z",
            "modified": "2022-03-30T12:50:48.720Z",
            "source_ref": "attack-pattern--aad9054d-a3c4-424e-9610-56667ff6cad4",
            "relationship_type": "subtechnique-of",
            "target_ref": "attack-pattern--5194796b-5f23-4524-8179-0e4cdbd1c34a"
        },
        {
            "name": "ENGAGE 1.0.0",
            "description": "Engage Navigator: atlas.mitre.org",
            "external_references": [
                {
                    "source_name": "mitre-atlas",
                    "url": "https://localhost:4200",
                    "external_id": "mitre-atlas"
                }
            ],
            "tactic_refs": [
                "x-mitre-tactic--798c2d7e-6da9-4d20-b01a-08dda01e4989",
                "x-mitre-tactic--d6430e09-9404-47d5-9731-9737f8d62bcb",
                "x-mitre-tactic--d9f7dc7f-48a4-4cf7-9417-ebb8df25fed9",
                "x-mitre-tactic--33e14ffb-49eb-4156-90df-782b9e44a3a3",
                "x-mitre-tactic--8fb30c8b-43fa-486b-b652-5bedc52328da",
                "x-mitre-tactic--350b0b74-19a6-4383-806e-f643f60fc829",
                "x-mitre-tactic--84e46311-9a2e-4a4c-a6ea-299e2f745152",
                "x-mitre-tactic--e6a7247f-3bd7-4c5e-be38-e57bca3fb7e3",
                "x-mitre-tactic--6b7f2588-7009-4462-a77a-7a7df7d4551a",
                "x-mitre-tactic--9a10021e-3678-4fde-a484-e415ee46ca90",
                "x-mitre-tactic--324dbcf8-660d-4a31-9d66-e068e09338ac",
                "x-mitre-tactic--55d3cb22-4581-46b2-82e4-a457a02e208a",
                "x-mitre-tactic--0e121e2a-2964-41b6-8ea7-2c3df266e816",
                "x-mitre-tactic--d1da7668-82eb-4a66-a7e1-ce3db73ab6b4",
                "x-mitre-tactic--00d566f1-21f0-4d27-b45b-7aa74b9fbd7b"
            ],
            "type": "x-mitre-matrix",
            "id": "x-mitre-matrix--73a011b4-9bc0-4f9e-9dc1-6416af11c4f3",
            "created": "2022-03-30T12:50:48.723Z",
            "modified": "2022-03-30T12:50:48.723Z"
        }
    ],
    "type": "bundle",
    "id": "bundle--8a8636ae-617d-4d86-9e5e-390d4e4deaa0",
    "spec_version": "2.0"
}